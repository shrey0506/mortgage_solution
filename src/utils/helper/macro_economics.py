from langchain.tools import tool
from langchain_core.messages import HumanMessage
from mortgage.src.model.gemini_20 import GeminiModelWrapper
from mortgage.src.utils.helper.boe_interest import fetch_latest_interest_rate

global model
model = GeminiModelWrapper

@tool
def get_macro_economics(user_details: str = None):
    """
    Provided the user details, checks for the macro economic condition of the user
    using Tooling Agent provided by Gemini model.

    Args:
        user_details (str): user data point.

    Returns:
        str: Risk score and suggested interest rate.
    """
    system_prompt = f"""You are a senior macroeconomics analyst.
    Given a user's job sector, assess their risk score and suggest an initial mortgage interest rate.
    Use the Bank of England base rate: {fetch_latest_interest_rate()}% and adjust based on the following occupational risk profiles:

    {{ "Category": "NHS/Healthcare", "Risk": "Grow", "Sensitivity": "Low", "Rate": "5.25%" }},
    {{ "Category": "Education/Public", "Risk": "Maintain", "Sensitivity": "Low-Medium", "Rate": "5.75%" }},
    {{ "Category": "Tech/Finance", "Risk": "Selective Grow", "Sensitivity": "Medium", "Rate": "6.00%" }},
    {{ "Category": "Construction/Transport", "Risk": "Restricted", "Sensitivity": "High", "Rate": "7.00%" }},
    {{ "Category": "Gig/Freelancers", "Risk": "Case-by-case", "Sensitivity": "High", "Rate": "7.50%" }}

    Adjust the rate based on macroeconomic conditions. Output format:
    ROI: <value>
    Risk_score: <Very High | High | Moderate | Low>
    """

    user_prompt = f"""
    You are a smart mortgage Assistant, Can you provide the initial/steer rate of interest range based on the provided user details? Also, provide the risk score as one of: Very High, High, Moderate, Low.
    Here are the user details: {user_details}

    Example:
    User Details: Works in Retail, moderate income, high debt.
    Response:
    ROI: 6.75
    Risk_score: High

    Now, based on the current user details, respond in the same format:
    ROI: <value>
    Risk_score: <value>
    """
    model.sys_instruct = system_prompt
    messages = [HumanMessage(content=user_prompt)]
    result = model.generate(messages)

    # Validate response
    if not result or not result.generations or not result.generations[0].message.content:
        return "Error: No response generated by the model."

    response = result.generations[0].message.content
    return response